# Scikit-learn Cookbook
- Chapter 1: High-Performance Machine Learning – NumPy
- Chapter 2: Pre-Model Workflow and Pre-Processing
- Chapter 3: Dimensionality Reduction
- Chapter 4: Linear Models with scikit-learn
- Chapter 5: Linear Models – Logistic Regression
- Chapter 6: Building Models with Distance Metrics
- Chapter 7: Cross-Validation and Post-Model Workflow
- Chapter 8: Support Vector Machines
- Chapter 9: Tree Algorithms and Ensembles
- Chapter 10: Text and Multiclass Classification with scikit-learn
- Chapter 11: Neural Networks
- Chapter 12: Create a Simple Estimator

# Chapter 1: High-Performance Machine Learning – NumPy
- NumPy basics
- Loading the iris dataset
- Viewing the iris dataset
- Viewing the iris dataset with pandas
- Plotting with NumPy and matplotlib
- A minimal machine learning recipe – SVM classification
- Introducing cross-validation
- Putting it all together
- Machine learning overview – classification versus regression

# Chapter 2: Pre-Model Workflow and Pre-Processing
- Creating sample data for toy analysis
- Scaling data to the standard normal distribution
- Creating binary features through thresholding
- Working with categorical variables
- Imputing missing values through various strategies
- A linear model in the presence of outliers
- Putting it all together with pipelines
- Using Gaussian processes for regression
- Using SGD for regression

# Chapter 3: Dimensionality Reduction
- Reducing dimensionality with PCA
- Using factor analysis for decomposition
- Using kernel PCA for nonlinear dimensionality reduction
- Using truncated SVD to reduce dimensionality
- Using decomposition to classify with DictionaryLearning
- Doing dimensionality reduction with manifolds – t-SNE
- Testing methods to reduce dimensionality with pipelines

# Chapter 4: Linear Models with scikit-learn
- Fitting a line through data
- Fitting a line through data with machine learning
- Evaluating the linear regression model
- Using ridge regression to overcome linear regression's shortfalls
- Optimizing the ridge regression parameter
- Using sparsity to regularize models
- Taking a more fundamental approach to regularization with LARS

# Chapter 5: Linear Models – Logistic Regression
- Loading data from the UCI repository
- Viewing the Pima Indians diabetes dataset with pandas
- Looking at the UCI Pima Indians dataset web page
- Machine learning with logistic regression
- Examining logistic regression errors with a confusion matrix
- Varying the classification threshold in logistic regression
- Receiver operating characteristic – ROC analysis
- Plotting an ROC curve without context
- Putting it all together – UCI breast cancer dataset

# Chapter 6: Building Models with Distance Metrics
- Using k-means to cluster data
- Optimizing the number of centroids
- Assessing cluster correctness
- Using MiniBatch k-means to handle more data
- Quantizing an image with k-means clustering
- Finding the closest objects in the feature space
- Probabilistic clustering with Gaussian Mixture Models
- Using k-means for outlier detection
- Using KNN for regression

# Chapter 7: Cross-Validation and Post-Model Workflow
- Selecting a model with cross-validation
- K-fold cross-validation
- Balanced cross-validation
- Cross-validation with ShuffleSplit
- Time series cross-validation
- Grid search with scikit-learn
- Randomized search with scikit-learn
- Classification metrics
- Regression metrics
- Clustering metrics
- Using dummy estimators to compare results
- Feature selection
- Feature selection on L1 norms
- Persisting models with joblib or pickle

# Chapter 8: Support Vector Machines
- Classifying data with a linear SVM
- Optimizing an SVM
- Multiclass classification with SVM
- Support vector regression

# Chapter 9: Tree Algorithms and Ensembles
- Doing basic classifications with decision trees
- Visualizing a decision tree with pydot
- Tuning a decision tree
- Using decision trees for regression
- Reducing overfitting with cross-validation
- Implementing random forest regression
- Bagging regression with nearest neighbor
- Tuning gradient boosting trees
- Tuning an AdaBoost regressor
- Writing a stacking aggregator with scikit-learn

# Chapter 10: Text and Multiclass Classification with scikit-learn
- Using LDA for classification
- Working with QDA – a nonlinear LDA
- Using SGD for classification
- Classifying documents with Naive Bayes
- Label propagation with semi-supervised learning

# Chapter 11: Neural Networks
- Perceptron classifier
- Neural network – multilayer perceptron
- Stacking with a neural network

# Chapter 12: Create a Simple Estimator
- Creating a simple estimator