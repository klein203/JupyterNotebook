{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlines\n",
    "## What's new\n",
    "- Sarsa Algorithm Implementation\n",
    "- Code refactor: extract class ReinforcementLearning as the ancestor of all the reinforcement algorithm\n",
    "- Re-document the comment of core algorithm\n",
    "\n",
    "## Keyword\n",
    "- Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "## General imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze 2D Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2D Maze Environment\n",
    "\"\"\"\n",
    "\n",
    "class Maze2DEnv(tk.Tk, object):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(Maze2DEnv, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self._name = config.get('name', 'Maze2D')\n",
    "        self.title(self._name)\n",
    "        \n",
    "        # render frequency, default 0.5s for rendering interval\n",
    "        self._refresh_interval = config.get('refresh_interval', 0.5)\n",
    "        \n",
    "        # unit width and height\n",
    "        self._unit_width = config.get('unit_pixel', 35)\n",
    "        self._unit_height = config.get('unit_pixel', 35)\n",
    "        \n",
    "        # 2D shape, eg: cols x rows: 5 x 4\n",
    "        self._shape = config.get('shape', (5, 4))\n",
    "        \n",
    "        # space for actions\n",
    "        self._action_space = ['left', 'right', 'up', 'down']\n",
    "        self._n_actions = len(self._action_space)\n",
    "        \n",
    "        screen_width, screen_height = self.winfo_screenwidth(), self.winfo_screenheight()\n",
    "        window_width, window_height = self._shape[0] * self._unit_width, self._shape[1] * self._unit_height\n",
    "        self.geometry('%dx%d+%d+%d' % (window_width, window_height, screen_width - window_width, screen_height- window_height))\n",
    "        \n",
    "        # init objects in maze\n",
    "        self._maze_objects = config.get('maze_objects')\n",
    "\n",
    "        # observation, default (0, 0) (from the very left/top position)\n",
    "        self._origin = config.get('origin', (0, 0))\n",
    "        self.obs = self._origin\n",
    "        \n",
    "        # draw maze\n",
    "        self._draw_maze();\n",
    "        \n",
    "#         self.bind(\"<Left>\", lambda e: self._move_obs(0))\n",
    "#         self.bind(\"<Right>\", lambda e: self._move_obs(1))\n",
    "#         self.bind(\"<Up>\", lambda e: self._move_obs(2))\n",
    "#         self.bind(\"<Down>\", lambda e: self._move_obs(3))\n",
    "    \n",
    "    def _draw_maze(self):\n",
    "        # canvas\n",
    "        self.canvas = tk.Canvas(self, bg='white',\n",
    "                                width=self._shape[0] * self._unit_width,\n",
    "                                height=self._shape[1] * self._unit_height)\n",
    "        self.canvas.pack()\n",
    "        self.redraw_all(self.canvas)\n",
    "        \n",
    "    def redraw_all(self, canvas):\n",
    "        # clear all objects on the canvas\n",
    "        canvas.delete(\"all\")\n",
    "        \n",
    "        # draw grids\n",
    "        for x in range(0, self._shape[0] * self._unit_width, self._unit_width):\n",
    "            x0, y0, x1, y1 = x, 0, x, self._shape[1] * self._unit_height\n",
    "            canvas.create_line(x0, y0, x1, y1)\n",
    "        for y in range(0, self._shape[1] * self._unit_height, self._unit_height):\n",
    "            x0, y0, x1, y1 = 0, y, self._shape[0] * self._unit_width, y\n",
    "            canvas.create_line(x0, y0, x1, y1)\n",
    "            \n",
    "        # draw mines, black square\n",
    "        # draw treasure, yellow square\n",
    "        for key, value in self._maze_objects.items():\n",
    "            color = 'black'\n",
    "            if value == 'treasure':\n",
    "                color = 'yellow'\n",
    "            elif value == 'mine':\n",
    "                color = 'black'\n",
    "                \n",
    "            canvas.create_rectangle(\n",
    "                key[0] * self._unit_width + 2, key[1] * self._unit_height + 2,\n",
    "                (key[0] + 1) * self._unit_width - 2, (key[1] + 1) * self._unit_height - 2,\n",
    "                fill=color)\n",
    "\n",
    "        # draw obs, grey square\n",
    "        canvas.create_rectangle(\n",
    "            self.obs[0] * self._unit_width + 2, self.obs[1] * self._unit_height + 2,\n",
    "            (self.obs[0] + 1) * self._unit_width - 2, (self.obs[1] + 1) * self._unit_height - 2,\n",
    "            fill='grey')\n",
    "        \n",
    "    def _conflict_check(self, obs):\n",
    "        conflict = obs in self._maze_objects.keys()\n",
    "        obj = self._maze_objects.get(obs, None)\n",
    "        return conflict, obj\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset observation\n",
    "        self.obs = self._origin\n",
    "        self.render()\n",
    "        return self.obs\n",
    "\n",
    "    def step(self, action):\n",
    "        # init\n",
    "        obs_ = list(self.obs)\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # action move, \n",
    "        if action == 0:  # 'left'\n",
    "            obs_[0] -= 1\n",
    "            if obs_[0] < 0:\n",
    "                obs_[0] = 0\n",
    "        elif action == 1:  # 'right'\n",
    "            obs_[0] += 1\n",
    "            if obs_[0] > self._shape[0] - 1:\n",
    "                obs_[0] = self._shape[0] - 1\n",
    "        elif action == 2:  # 'up'\n",
    "            obs_[1] -= 1\n",
    "            if obs_[1] < 0:\n",
    "                obs_[1] = 0\n",
    "        elif action == 3:  # 'down'\n",
    "            obs_[1] += 1\n",
    "            if obs_[1] > self._shape[1] - 1:\n",
    "                obs_[1] = self._shape[1] - 1\n",
    "        else:\n",
    "            raise Exception('invalid action code', action)\n",
    "\n",
    "        # conflict check\n",
    "        obs_ = tuple(obs_)\n",
    "        conflict, obj = self._conflict_check(obs_)\n",
    "        if conflict:\n",
    "            if obj == 'treasure':\n",
    "                obs_ = obj\n",
    "                reward = 1\n",
    "                done = True\n",
    "            elif obj == 'mine':\n",
    "                obs_ = obj\n",
    "                reward = -1\n",
    "                done = True\n",
    "            else:\n",
    "                raise Exception('invalid object', obj)\n",
    "        \n",
    "        self.obs = obs_\n",
    "        \n",
    "        return obs_, reward, done\n",
    "        \n",
    "    def render(self):\n",
    "        time.sleep(self._refresh_interval)\n",
    "        self.redraw_all(self.canvas)\n",
    "        self.update()\n",
    "    \n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return self._n_actions\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ancestor of all reinforcement learnigng algorithm\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class ReinforcementLearning(object):\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, eplison_greedy=0.9, *args, **kwargs):\n",
    "        self._name = 'Reinforcement Learning'\n",
    "        \n",
    "        # init available actions\n",
    "        self._actions = actions\n",
    "        \n",
    "        # init learning paras\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._eplison_greedy = eplison_greedy\n",
    "        \n",
    "        # init Q table\n",
    "        self._q_table = pd.DataFrame(columns=actions, dtype=np.float)\n",
    "    \n",
    "    def _check_state_available(self, obs):\n",
    "        # add obs to q_table if q_table does not contain it\n",
    "        if obs not in self._q_table.index:\n",
    "            self._q_table = self._q_table.append(\n",
    "                pd.Series(\n",
    "                    [0] * len(self._actions),\n",
    "                    index=self._q_table.columns,\n",
    "                    name=obs\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        # type of obs should be str\n",
    "        self._check_state_available(obs)\n",
    "        \n",
    "        # e-greedy policy for choosing action\n",
    "        if np.random.uniform() < self._eplison_greedy:\n",
    "            # p < 0.9, choose maxQ(s_, a_) for next move\n",
    "            obs_actions = self._q_table.loc[obs, :]\n",
    "            action = np.random.choice(obs_actions[obs_actions == np.max(obs_actions)].index)\n",
    "        else:\n",
    "            # p >= 0.9, choose random move\n",
    "            action = np.random.choice(self._actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, *args):\n",
    "        raise Exception('function should be implemented')\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self._q_table.__str__()\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    def save(self, filename):\n",
    "        self._q_table.to_csv(filename)\n",
    "        logging.debug('save weight to file %s:\\n%s' % (filename, self._q_table))\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self._q_table = pd.read_csv(filename, index_col=0, names=self._actions, header=0)\n",
    "        logging.debug('load weight from file %s:\\n%s' % (filename, self._q_table))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Agent with Q Learning algorithm\n",
    "\n",
    "Q(s, a) <- Q(s, a) + alpha * (R + gamma * maxQ(s_, a_) - Q(s, a))\n",
    "s <- s_\n",
    "\n",
    "alpha: learning rate\n",
    "gamma: reward decay\n",
    "\n",
    "q_target: R + gamma * maxQ(s_, a_)\n",
    "q_predict: Q(s, a)\n",
    "\"\"\"\n",
    "\n",
    "class QLearning(ReinforcementLearning):\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, eplison_greedy=0.9, *args, **kwargs):\n",
    "        super(QLearning, self).__init__(actions, alpha, gamma, eplison_greedy, *args, **kwargs)\n",
    "        self._name = 'Q Learning'\n",
    "\n",
    "    def learn(self, obs, a, r, obs_):\n",
    "        self._check_state_available(obs_)\n",
    "        \n",
    "        if obs_ in ['treasure', 'mine']:\n",
    "            q_target = r\n",
    "        else:\n",
    "            q_target = r + self._gamma * self._q_table.loc[obs_, :].max()\n",
    "        \n",
    "        q_predict = self._q_table.loc[obs, a]\n",
    "        \n",
    "        self._q_table.loc[obs, a] = q_predict + self._alpha * (q_target - q_predict)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Agent with Sarsa algorithm\n",
    "\n",
    "Q(s, a) <- Q(s, a) + alpha * (R + gamma * Q(s_, a_) - Q(s, a))\n",
    "s <- s_\n",
    "a <- a_\n",
    "\n",
    "alpha: learning rate\n",
    "gamma: reward decay\n",
    "\n",
    "q_target: R + gamma * Q(s_, a_)\n",
    "q_predict: Q(s, a)\n",
    "\"\"\"\n",
    "\n",
    "class Sarsa(ReinforcementLearning):\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, eplison_greedy=0.9, *args, **kwargs):\n",
    "        super(Sarsa, self).__init__(actions, alpha, gamma, eplison_greedy, *args, **kwargs)\n",
    "        self._name = 'Sarsa'\n",
    "    \n",
    "    def learn(self, obs, a, r, obs_, a_):\n",
    "        self._check_state_available(obs_)\n",
    "        \n",
    "        if obs_ in ['treasure', 'mine']:\n",
    "            q_target = r\n",
    "        else:\n",
    "            q_target = r + self._gamma * self._q_table.loc[obs_, a_]\n",
    "        \n",
    "        q_predict = self._q_table.loc[obs, a]\n",
    "        \n",
    "        self._q_table.loc[obs, a] = q_predict + self._alpha * (q_target - q_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def q_learning_train(config, env, agent):\n",
    "    n_episode = config.get('n_episode', 100)\n",
    "    n_step_per_episode = config.get('n_step_per_episode', 100)\n",
    "    weight_filename = config.get('weight_filename', 'q_learning_weight')\n",
    "    \n",
    "    # Repeat (for each of episode)\n",
    "    for i_episode in range(n_episode):\n",
    "        logging.info(\"[Episode %d]\" % (i_episode + 1))\n",
    "\n",
    "        # initialize s\n",
    "        obs = env.reset()\n",
    "        \n",
    "        # Repeat (for each step of episode)\n",
    "        for i_step in range(n_step_per_episode):\n",
    "            env.render()\n",
    "\n",
    "            # choose a from s using policy derived from Q (eg: e-greedy)\n",
    "            action = agent.choose_action(str(obs))\n",
    "\n",
    "            # take action a, observe s_, r\n",
    "            obs_, reward, done = env.step(action)\n",
    "\n",
    "            # update Q table\n",
    "            agent.learn(str(obs), action, reward, str(obs_))\n",
    "\n",
    "            # update s\n",
    "            obs = obs_\n",
    "\n",
    "            # terminal check\n",
    "            if done or i_step == n_step_per_episode - 1:\n",
    "                if done:\n",
    "                    if reward > 0:\n",
    "                        disp_state = 'WIN'\n",
    "                    else:\n",
    "                        disp_state = 'BUSTED'\n",
    "                else:\n",
    "                    disp_state = 'LOST'\n",
    "                \n",
    "                logging.info('Episode %d terminated with %d steps, finally %s' % (i_episode + 1, i_step + 1, disp_state))\n",
    "                logging.info('%s:\\n%s\\n' % (agent.name, agent))\n",
    "                break\n",
    "                \n",
    "    env.destroy()    \n",
    "    agent.save(''.join(weight_filename))\n",
    "\n",
    "# configs\n",
    "q_learning_train_config = {\n",
    "    'n_episode': 200,\n",
    "    'n_step_per_episode': 100,\n",
    "    'weight_filename': '5x4_Ep200_St100_Q_Learning.csv'\n",
    "}\n",
    "\n",
    "maze_config = {\n",
    "    'name': 'Maze2D',\n",
    "    'shape': (5, 4),\n",
    "    'maze_objects': {\n",
    "        (1, 1): 'mine',\n",
    "        (1, 2): 'mine',\n",
    "        (2, 1): 'mine',\n",
    "        (3, 3): 'mine',\n",
    "        (4, 1): 'mine',\n",
    "        (4, 3): 'treasure',  # treasure\n",
    "    },\n",
    "    'origin': (0, 0),\n",
    "    'refresh_interval': 0.05,\n",
    "    'unit_pixel': 35,\n",
    "}\n",
    "\n",
    "# train process\n",
    "# Q Learning\n",
    "env = Maze2DEnv(config=maze_config)\n",
    "agent = QLearning(list(range(env.n_actions)))\n",
    "env.after(100, q_learning_train, q_learning_train_config, env, agent)\n",
    "env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def sarsa_train(config, env, agent):\n",
    "    n_episode = config.get('n_episode', 100)\n",
    "    n_step_per_episode = config.get('n_step_per_episode', 100)\n",
    "    weight_filename = config.get('weight_filename', 'sarsa_weight')\n",
    "    \n",
    "    # Repeat (for each episode)\n",
    "    for i_episode in range(n_episode):\n",
    "        logging.info(\"[Episode %d]\" % (i_episode + 1))\n",
    "\n",
    "        # initialize s\n",
    "        obs = env.reset()\n",
    "        \n",
    "        # choose a from s using policy derived from Q (eg: e-greedy)\n",
    "        action = agent.choose_action(str(obs))\n",
    "        \n",
    "        # Repeat (for each step of episode) \n",
    "        for i_step in range(n_step_per_episode):\n",
    "            env.render()\n",
    "\n",
    "            # take action a, observe s_, r\n",
    "            obs_, reward, done = env.step(action)\n",
    "\n",
    "            # choose a_ from s_ using policy derived from Q (eg: e-greedy)\n",
    "            action_ = agent.choose_action(str(obs_))\n",
    "            \n",
    "            # update Q table\n",
    "            agent.learn(str(obs), action, reward, str(obs_), action_)\n",
    "\n",
    "            # update s\n",
    "            obs = obs_\n",
    "            \n",
    "            # update a\n",
    "            action = action_\n",
    "\n",
    "            # terminal check\n",
    "            if done or i_step == n_step_per_episode - 1:\n",
    "                if done:\n",
    "                    if reward > 0:\n",
    "                        disp_state = 'WIN'\n",
    "                    else:\n",
    "                        disp_state = 'BUSTED'\n",
    "                else:\n",
    "                    disp_state = 'LOST'\n",
    "                \n",
    "                logging.info('Episode %d terminated with %d steps, finally %s' % (i_episode + 1, i_step + 1, disp_state))\n",
    "                logging.info('%s:\\n%s\\n' % (agent.name, agent))\n",
    "                break\n",
    "                \n",
    "    env.destroy()    \n",
    "    agent.save(''.join(weight_filename))\n",
    "\n",
    "\n",
    "# configs\n",
    "sarsa_train_config = {\n",
    "    'n_episode': 100,\n",
    "    'n_step_per_episode': 200,\n",
    "    'weight_filename': '5x4_Ep100_St200_Sarsa.csv'\n",
    "}\n",
    "\n",
    "maze_config = {\n",
    "    'name': 'Maze2D',\n",
    "    'shape': (5, 4),\n",
    "    'maze_objects': {\n",
    "        (1, 1): 'mine',\n",
    "        (1, 2): 'mine',\n",
    "        (2, 1): 'mine',\n",
    "        (3, 3): 'mine',\n",
    "        (4, 1): 'mine',\n",
    "        (4, 3): 'treasure',  # treasure\n",
    "    },\n",
    "    'origin': (0, 0),\n",
    "    'refresh_interval': 0.05,\n",
    "    'unit_pixel': 35,\n",
    "}\n",
    "\n",
    "# train process\n",
    "# Sarsa\n",
    "env = Maze2DEnv(config=maze_config)\n",
    "agent = Sarsa(list(range(env.n_actions)))\n",
    "env.after(100, sarsa_train, sarsa_train_config, env, agent)\n",
    "env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run\n",
    "def test(config, env, agent):\n",
    "    logging.info('Simulation')\n",
    "    obs = env.reset()\n",
    "    i_step = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        i_step += 1\n",
    "        action = agent.choose_action(str(obs))\n",
    "        obs_, reward, done = env.step(action)\n",
    "        \n",
    "        # skip learning phrase\n",
    "        obs = obs_\n",
    "        \n",
    "        if done:\n",
    "            if reward > 0:\n",
    "                disp_state = 'WIN'\n",
    "            else:\n",
    "                disp_state = 'BUSTED'\n",
    "            \n",
    "            logging.info('Terminated with %d steps, finally %s' % (i_step, disp_state))\n",
    "            break\n",
    "    \n",
    "    env.destroy()\n",
    "\n",
    "# configs\n",
    "maze_config = {\n",
    "    'name': 'Maze2D',\n",
    "    'shape': (5, 4),\n",
    "    'maze_objects': {\n",
    "        (1, 1): 'mine',\n",
    "        (1, 2): 'mine',\n",
    "        (2, 1): 'mine',\n",
    "        (3, 3): 'mine',\n",
    "        (4, 1): 'mine',\n",
    "        (4, 3): 'treasure',  # treasure\n",
    "    },\n",
    "    'origin': (0, 0),\n",
    "    'refresh_interval': 0.5,\n",
    "    'unit_pixel': 35,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation process\n",
    "# Q Learning\n",
    "env = Maze2DEnv(config=maze_config)\n",
    "agent = QLearning(list(range(env.n_actions)))\n",
    "agent.load(''.join(q_learning_train_config.get('weight_filename')))\n",
    "\n",
    "env.after(100, test, None, env, agent)\n",
    "env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa\n",
    "env = Maze2DEnv(config=maze_config)\n",
    "agent = Sarsa(list(range(env.n_actions)))\n",
    "agent.load(''.join(sarsa_train_config.get('weight_filename')))\n",
    "\n",
    "env.after(100, test, None, env, agent)\n",
    "env.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
