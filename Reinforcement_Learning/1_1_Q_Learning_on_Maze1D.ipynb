{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1D Maze Environment\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Maze1DEnv(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # render frequency, default 0.5s for rendering interval\n",
    "        self._refresh_interval = kwargs.get('refresh_interval', 0.5)\n",
    "        \n",
    "        # 1D maze length, default 5\n",
    "        self._n_states = kwargs.get('n_states', 5)\n",
    "        \n",
    "        # space for actions\n",
    "        self._action_space = ['left', 'right']\n",
    "        self._n_actions = len(self._action_space)\n",
    "        \n",
    "        # observation, default 0 (from the very left side)\n",
    "        self.obs = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        # reset observation\n",
    "        self.obs = 0\n",
    "        return self.obs\n",
    "\n",
    "    def step(self, action):\n",
    "        # init\n",
    "        obs_ = self.obs\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # action move, \n",
    "        if action == 0:  # 0 for self._action_space[0], 'left'\n",
    "            obs_ -= 1\n",
    "            if obs_ < 0:\n",
    "                obs_ = 0\n",
    "        elif action == 1:  # 1 for self._action_space[1], 'right'\n",
    "            obs_ += 1\n",
    "            if obs_ == self._n_states - 1:\n",
    "                obs_ = 'treasure'\n",
    "                reward = 1\n",
    "                done = True\n",
    "        else:\n",
    "            raise Exception('invalid action code', action)\n",
    "\n",
    "        # update observation (next state)\n",
    "        self.obs = obs_\n",
    "        \n",
    "        return obs_, reward, done\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def render(self):\n",
    "        # init, display like '------T', T for 'treasure'\n",
    "        env_list = ['_'] * (self._n_states - 1) + ['T']\n",
    "        \n",
    "        if self.obs == 'treasure':\n",
    "            sys.stdout.write('\\r%s WIN' % ''.join(env_list))\n",
    "        else:\n",
    "            # self.obs is offset of 1D maze\n",
    "            env_list[self.obs] = '-'\n",
    "            sys.stdout.write('\\r%s' % ''.join(env_list))\n",
    "        \n",
    "        time.sleep(self._refresh_interval)\n",
    "    \n",
    "    @property\n",
    "    def n_actions(self):\n",
    "        return self._n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agent with Q Learning algorithm\n",
    "\n",
    "Q(s, a) <- Q(s, a) + alpha * (R + gamma * maxQ(s_, a_) - Q(s, a))\n",
    "s <- s_\n",
    "\n",
    "alpha: learning rate\n",
    "gamma: reward decay\n",
    "\n",
    "q_target: R + gamma * maxQ(s_, a_)\n",
    "q_predict: Q(s, a)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class QLearning(object):\n",
    "    def __init__(self, actions, alpha=0.1, gamma=0.9, eplison_greedy=0.9, *args, **kwargs):\n",
    "        # init available actions\n",
    "        self._actions = actions\n",
    "        \n",
    "        # init learning paras\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._eplison_greedy = eplison_greedy\n",
    "        \n",
    "        # init Q table\n",
    "        self._q_table = pd.DataFrame(columns=actions, dtype=np.float)\n",
    "    \n",
    "    def _check_state_available(self, obs):\n",
    "        # add obs to q_table if q_table does not contain it\n",
    "        if obs not in self._q_table.index:\n",
    "            self._q_table = self._q_table.append(\n",
    "                pd.Series(\n",
    "                    [0] * len(self._actions),\n",
    "                    index=self._q_table.columns,\n",
    "                    name=obs\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def choose_action(self, obs):\n",
    "        self._check_state_available(obs)\n",
    "        \n",
    "        # e-greedy policy for choosing action\n",
    "        if np.random.uniform() < self._eplison_greedy:\n",
    "            # p < 0.9, choose maxQ(s_, a_) for next move\n",
    "            obs_actions = self._q_table.loc[obs, :]\n",
    "            action = np.random.choice(obs_actions[obs_actions == np.max(obs_actions)].index)\n",
    "        else:\n",
    "            # p >= 0.9, choose random move\n",
    "            action = np.random.choice(self._actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, obs, a, r, obs_):\n",
    "        self._check_state_available(obs_)\n",
    "        \n",
    "        if obs_ != 'treasure':\n",
    "            q_target = r + self._gamma * self._q_table.loc[obs_, :].max()\n",
    "        else:\n",
    "            q_target = r\n",
    "        \n",
    "        q_predict = self._q_table.loc[obs, a]\n",
    "        \n",
    "        self._q_table.loc[obs, a] = q_predict + self._alpha * (q_target - q_predict)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self._q_table.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import sys\n",
    "# import time\n",
    "# import Maze1DEnv\n",
    "# import QLearning\n",
    "\n",
    "N_EPISODE = 100\n",
    "N_MAX_STEP_PER_EPISODE = 30\n",
    "\n",
    "env = Maze1DEnv(n_states=6, refresh_interval=0.1)\n",
    "rl = QLearning(list(range(env.n_actions)))\n",
    "\n",
    "# N_EPISODE episode loop\n",
    "for i_episode in range(N_EPISODE):\n",
    "    print(\"[Episode %d]\" % (i_episode + 1))\n",
    "    \n",
    "    obs = env.reset()\n",
    "    # N_MAX_STEP_PER_EPISODE step loop\n",
    "    for i_step in range(N_MAX_STEP_PER_EPISODE):\n",
    "#         print(\"\\n[Step %d]\" % (i_step + 1))\n",
    "        env.render()\n",
    "        \n",
    "        # choose next action\n",
    "        action = rl.choose_action(str(obs))\n",
    "        \n",
    "        # move and feedback from env\n",
    "        obs_, reward, done = env.step(action)\n",
    "        \n",
    "        # update q_table\n",
    "        rl.learn(str(obs), action, reward, str(obs_))\n",
    "        \n",
    "        obs = obs_\n",
    "        \n",
    "        # show q_table\n",
    "        if done or i_step == N_MAX_STEP_PER_EPISODE - 1:\n",
    "            print('\\n%s' % rl)\n",
    "        \n",
    "        # earlier out loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
